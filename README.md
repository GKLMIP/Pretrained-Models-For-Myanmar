# Pretrained-Models-For-Myanmar

## Introduction
Our aims in this paper are to train a monolingual pre-trained model for the Myan-mar language and improve the state-of-the-art in its NLP tasks including Part-of-speech tagging and news classification. We presented transformer-based PTMs with four versions: BERT-small, BERT-base, ELECTRA-small and ELECTRA-base. Besides, we crawler a set of comments (total of 3566 comments) on electronic products of shopping(https://www.shop.com.mm/) website. For more details of our dataset or our models, please see our paper “Pre-trained Models and Evaluation Data for the Myanmar Language”.

## Citation
If you use our models or our dataset, please consider citing our paper:
```
@InProceedings{,
author="Jiang, Shengyi
and Huang, Xiuwen
and Cai, Xiaonan
and Lin, Nankai",
title="Pre-trained Models and Evaluation Data for the Myanmar Language",
booktitle="The 28th International Conference on Neural Information Processing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
}
